2022-11-06
7：00
起床 早饭 3
-10：40
吴恩达视频若干

昨天与周学长交流将近1个小时，今天上午又与段学长交流许久
怎么将词语拆分成最小粒度还不好做 bpe似乎更侧重词频 但我这里更侧重词语内部的attention
可是如果整个句子全部按字符拆，就会导致序列过长。
我的想法是能不能用attention作卷积核的cnn来具体做每个单词
学长不断强调这样似乎会扫一个句子太多次，同时模型过于复杂
我发觉我的想法实在是很模糊
想法虽然与现有的在序列上做cnn，中间语言等多有不同
但一旦具体到实际上就会很不清楚，不知如何进行

退而求其次，这个想法对于transformer的可解释性上，似乎也不能起到很好作用。
因为transformer正是去掉了一切人工先验想法自行学习才有的这样好的结果
-11：00
吴恩达 P105 结束
-12：00
与段老师 讨论 
他提到WSD（词义消歧）tree-lstm  我这好像是syntax base
吃饭 自选 12
查看opennmt的transformer代码
12：20
搞并行，搞不了
-14：00
睡午觉 睡不着 
堕落啊
回实验室了
-16：30
吴恩达 李沐 视频若干
MoCo论文一篇
-19：30
可乐鸡 15
死侍1
20：00
洗澡结束，零食饮料18
22：30
死侍2看完




