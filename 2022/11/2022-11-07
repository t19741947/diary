2022-11-07
7：00
醒
8：20
起床
8：40
做完核酸 开始吃早饭 八宝粥5
-11：00
改代码 看吴恩达
代码改了一上午 改了个寂寞
还不如去写点模型跑跑
-11：30
久违的烤肠炒饭 12
-13：00
改代码 看李沐 吴恩达 视频 
论文半篇
13：30-16：00
专业写作
-17：40
与zjp学长讨论至少1h关于我的想法 
有种茅塞顿开的感觉
学长不愧是学长，明天就拉上学长给段老师重新讲一遍，但可惜学长婉拒了
许多细节和实现上的内容在不断落实
（学长还夸了我好多，说创新点肯定够了，
还说许多顶会论文，别人正在做的事大部分都是无意义的，而我这个想法不一样
还说，我的想法与段老师一直以来的想法，一直想做的东西是一致的
但也发现出了许多实现上的问题，我们花了很长时间试图把这个想法削减出一个简单系统甚至是一两个子任务，但是失败了）
我感觉我的这个想法更像是一个半成品
吃晚饭 方塔 11
-20：00
跑步半小时 洗澡半小时 面包 6

一直想学长问的loss怎么算，以及具体做法
比如噪声、掩码、有监督
在跑步时想到，attention，从人类的角度出发，其目的是什么
比如动物的语音序列信息（一切时间信息都可以看作序列信息，非序列也能做）
那就是如何更快地理解、反应，也就是系统性能。如何用最少的内容理解。（存留序列以防止被忽略的细节信息）
那么继续拓展，attention最主要的目的，就是下游任务
那就应该抛弃一直以来的无监督想法，用一点有监督的下游任务的表现性能来做loss
但这样是否要训练的内容太多，要解释的内容是否还能一起做还是另外单独，是否会过于复杂

多语言映射到一个中间语言表示 多模态！

因果关系 因果模型

prompt

bert 模型非常的健壮或者是冗余度很高，直接去掉一整层的 attention-head 并不会对模型的最终表现有太大的影响。 直接去掉整层的 attention-head 模型表现并没有大幅度的下降，说明各层提取的特征信息并不是一层一层的串行传递到分类器的，而是通过残差连接直接传导到对应的层。
https://zhuanlan.zhihu.com/p/148729018
现有的机器翻译nlp等等，有种做复杂了的感觉，或许未来，机器理解可以用回这些复杂的东西

https://zhuanlan.zhihu.com/p/287126616
https://www.connectedpapers.com/main/95a251513853c6032bdecebd4b74e15795662986/What-Does-BERT-Look-at%3F-An-Analysis-of-BERT%E2%80%99s-Attention/graph

-22：00
这一天接触的论文比我开学以来一个多月还多
回宿舍洗衣裳


